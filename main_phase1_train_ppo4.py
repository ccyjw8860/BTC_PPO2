import os
import sys
import numpy as np
import matplotlib.pyplot as plt
import gymnasium as gym

from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback, CallbackList
from utils.models.ppo_cnn2 import Custom1DCNN

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from utils.rl_data.trading_env3 import TradingEnv 
from utils.rl_data.data_generator import RLDataGenerator
from datetime import datetime

def get_current_datetime():
    return datetime.now().strftime("%Y%m%d_%H%M")

# -----------------------------------------------------------------------------
# Callbacks (ê¸°ì¡´ê³¼ ë™ì¼)
# -----------------------------------------------------------------------------
class TensorboardCallback(BaseCallback):
    def __init__(self, verbose=0):
        super().__init__(verbose)

    def _on_step(self) -> bool:
        infos = self.locals.get("infos", [{}])[0]
        if "equity_usd" in infos:
            self.logger.record("custom/equity_usd", infos["equity_usd"])
        if "log_equity" in infos:
            self.logger.record("custom/log_equity", infos["log_equity"])
        if "open_position_length" in infos:
            self.logger.record("custom/open_position_length", infos["open_position_length"])
        return True

class CustomEvalCallback(BaseCallback):
    """
    í‰ê°€ í™˜ê²½ì—ì„œ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•˜ê³ , 
    1. TensorBoardì— ìì‚°(Equity) ì •ë³´ë¥¼ ê¸°ë¡í•˜ë©°
    2. 'í‰ê·  ìµœì¢… ìì‚°(Mean Final Equity)'ì´ ê°€ì¥ ë†’ì„ ë•Œ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤.
    """
    def __init__(self, eval_env, check_freq: int, log_dir: str, 
                 n_eval_episodes: int = 5, 
                 best_model_save_path: str = None, 
                 verbose=1):
        super(CustomEvalCallback, self).__init__(verbose)
        self.eval_env = eval_env
        self.check_freq = check_freq
        self.log_dir = log_dir
        self.n_eval_episodes = n_eval_episodes
        self.best_model_save_path = best_model_save_path
        self.best_mean_equity = -np.inf

    def _init_callback(self) -> None:
        if self.best_model_save_path is not None:
            os.makedirs(self.best_model_save_path, exist_ok=True)

    def _on_step(self) -> bool:
        if self.n_calls % self.check_freq == 0:
            total_equity = 0.0
            valid_episodes = 0
            
            # --- [1] í‰ê°€ ë£¨í”„ (n_eval_episodes ë§Œí¼ ë°˜ë³µ) ---
            for _ in range(self.n_eval_episodes):
                obs = self.eval_env.reset()
                done = False
                while not done:
                    # Deterministic=Trueë¡œ í‰ê°€ (í™•ë¥ ì  ìš”ì†Œ ì œê±°)
                    action, _ = self.model.predict(obs, deterministic=True)
                    obs, reward, done, infos = self.eval_env.step(action)
                    
                    if done:
                        info = infos[0]
                        current_equity = 0.0
                        
                        # Infoì—ì„œ equity_usd ì¶”ì¶œ
                        if 'equity_usd' in info:
                            current_equity = info['equity_usd']
                        elif 'terminal_observation' in info and 'equity_usd' in info.get('terminal_info', {}):
                            current_equity = info['terminal_info']['equity_usd']
                            
                        total_equity += current_equity
                        valid_episodes += 1
            
            # --- [2] ê²°ê³¼ ê³„ì‚° ë° ë¡œê¹… ---
            if valid_episodes > 0:
                # ì—¬ëŸ¬ íŒì˜ 'í‰ê· 'ì„ ìµœì¢… ì„±ëŠ¥ìœ¼ë¡œ ê°„ì£¼ (ë” ì•ˆì •ì ì„)
                mean_equity = total_equity / valid_episodes
                
                # ğŸŸ¢ [ë¹ ì§„ ë¶€ë¶„ ì¶”ê°€] TensorBoard ê¸°ë¡
                # ê¸°ì¡´ ê·¸ë˜í”„ì™€ ì´ì–´ì§€ë„ë¡ íƒœê·¸ëª…ì„ ë§ì¶°ì¤ë‹ˆë‹¤.
                self.logger.record("eval/final_equity_usd", mean_equity)
                self.logger.record("eval/log_final_equity", np.log(max(mean_equity, 1e-6)))
                
                if self.verbose > 0:
                    print(f"Eval at step {self.num_timesteps}: Mean Equity = ${mean_equity:,.2f}")

                # --- [3] Best Model ì €ì¥ ë¡œì§ ---
                if self.best_model_save_path is not None:
                    if mean_equity > self.best_mean_equity:
                        if self.verbose > 0:
                            print(f"ğŸš€ New Best Model! (Equity: ${self.best_mean_equity:,.2f} -> ${mean_equity:,.2f})")
                        
                        self.best_mean_equity = mean_equity
                        
                        # ëª¨ë¸ ì €ì¥
                        save_path = os.path.join(self.best_model_save_path, "best_model_equity")
                        self.model.save(save_path)
                    
                    # í˜„ì¬ Best Score ê¸°ë¡
                    self.logger.record("eval/best_equity_usd", self.best_mean_equity)

        return True

# -----------------------------------------------------------------------------
# Environment Factory
# -----------------------------------------------------------------------------
def make_env(mode='train', sl_opts=None, tp_opts=None, window_size=100, max_episode_steps=2048, fee_rate=0.0005, slippage_rate=0.0):
    data_gen = RLDataGenerator(mode=mode, seq_len=window_size)
    env = TradingEnv(
        data_generator=data_gen,
        sl_options=sl_opts,
        tp_options=tp_opts,
        window_size=window_size,
        pip_value=1.0,
        lot_size=0.1,
        spread_pips=10.0,
        fee_rate=fee_rate,          
        slippage_rate=slippage_rate, 
        reward_scale=1.0,
        initial_balance=10000.0,
        max_episode_steps=max_episode_steps,
        mode=mode
    )
    return env

def evaluate_model(model: PPO, eval_env: VecNormalize, deterministic: bool = True):
    eval_env.training = False
    eval_env.norm_reward = False
    obs = eval_env.reset()
    equity_curve = [10000.0]
    done = False
    while not done:
        action, _ = model.predict(obs, deterministic=deterministic)
        obs, rewards, dones, infos = eval_env.step(action)
        equity_curve.append(infos[0].get("equity_usd", equity_curve[-1]))
        done = dones[0]
    return equity_curve, float(equity_curve[-1])

# -----------------------------------------------------------------------------
# Main Fine-tuning Loop
# -----------------------------------------------------------------------------
def main():
    # 1. ê²½ë¡œ ë° íŒŒë¼ë¯¸í„° ì„¤ì •
    # ìœˆë„ìš° ê²½ë¡œì¸ ê²½ìš° r"" string ì‚¬ìš© ê¶Œì¥
    PRETRAINED_PATH = r"./checkpoints/fee_rate_slippage_zero/best_last_model/best_model.zip"
    
    # [ì„¤ì •] ì¬í•™ìŠµ íŒŒë¼ë¯¸í„°
    FEE_RATE = 0.00025       # 0.025%
    SLIPPAGE_RATE = 0.0001   # ìŠ¬ë¦¬í”¼ì§€ë„ ì•„ì£¼ ì‚´ì§ (0.01%) ë„£ì–´ì£¼ëŠ” ê²Œ í˜„ì‹¤ì ì„ (ì„ íƒ)
    LEARNING_RATE = 3e-5     # ê¸°ì¡´ 3e-4 -> 3e-5 (1/10 ê°ì†Œ)
    TOTAL_TIMESTEPS = 20_000_000 # 2ì²œë§Œ ìŠ¤í… (í•„ìš”ì— ë”°ë¼ ì¡°ì ˆ)
    
    # ê¸°íƒ€ ì„¤ì •
    SL_OPTS = [0.002, 0.005, 0.01, 0.02] 
    TP_OPTS = [0.005, 0.01, 0.02, 0.04]
    WINDOW_SIZE = 1000
    NUM_ENVS = 8
    EPISODE_LENGTH = 2048
    
    current_time = get_current_datetime()
    # ğŸŸ¢ ìƒˆë¡œìš´ ì €ì¥ ê²½ë¡œ ìƒì„±
    CHECKPOINT_DIR = f"./checkpoints/finetune_fee_0_025_{current_time}"
    os.makedirs(CHECKPOINT_DIR, exist_ok=True)
    
    print(f"ğŸ”„ Loading Model from: {PRETRAINED_PATH}")
    print(f"âš™ï¸ Fine-tuning Settings -> Fee: {FEE_RATE*100}%, LR: {LEARNING_RATE}")
    print(f"ğŸ“‚ New Checkpoint Dir: {CHECKPOINT_DIR}")

    # 2. í™˜ê²½ ìƒì„± (ìƒˆë¡œìš´ Fee Rate ì ìš©)
    env_fns = [lambda: make_env('train', SL_OPTS, TP_OPTS, WINDOW_SIZE, EPISODE_LENGTH, FEE_RATE, SLIPPAGE_RATE) for _ in range(NUM_ENVS)]
    train_vec_env = DummyVecEnv(env_fns)
    # NormObs=Falseì´ë¯€ë¡œ í†µê³„ íŒŒì¼ ë¡œë“œ ì—†ì´ ìƒˆë¡œ ë§Œë“¤ì–´ë„ ê´œì°®ìŒ (Policy ì…ë ¥ ë¶„í¬ëŠ” ë™ì¼)
    train_env = VecNormalize(train_vec_env, norm_obs=False, norm_reward=True, clip_obs=100.0)

    # í…ŒìŠ¤íŠ¸ í™˜ê²½
    test_vec_env = DummyVecEnv([lambda: make_env('test', SL_OPTS, TP_OPTS, WINDOW_SIZE, max_episode_steps=0, fee_rate=FEE_RATE, slippage_rate=SLIPPAGE_RATE)])
    test_env = VecNormalize(test_vec_env, norm_obs=False, norm_reward=False, clip_obs=100.0, training=False)

    # 3. ëª¨ë¸ ë¡œë“œ (Fine-tuning ëª¨ë“œ)
    # [ì¤‘ìš”] custom_objectsë¥¼ í†µí•´ Optimizerì˜ LR ìŠ¤ì¼€ì¤„ëŸ¬ ë“±ì„ ë®ì–´ì“¸ ìˆ˜ë„ ìˆì§€ë§Œ,
    # SB3ì˜ load í•¨ìˆ˜ì— learning_rate ì¸ìë¥¼ ì£¼ë©´ ìƒˆë¡œìš´ LRë¡œ ì„¤ì •ë©ë‹ˆë‹¤.
    try:
        model = PPO.load(
            PRETRAINED_PATH,
            env=train_env,                  # ìƒˆë¡œìš´ í™˜ê²½(Fee ì ìš©ë¨) ì—°ê²°
            learning_rate=LEARNING_RATE,    # ë‚®ì¶˜ í•™ìŠµë¥  ì ìš©
            tensorboard_log="./tensorboard_log/", # ë¡œê·¸ í´ë” ë¶„ë¦¬
            custom_objects={'learning_rate': LEARNING_RATE} # ì•ˆì „ì¥ì¹˜
        )
        print("âœ… Model loaded successfully!")
    except Exception as e:
        print(f"âŒ Error loading model: {e}")
        return

    # 4. ì½œë°± ì„¤ì •
    checkpoint_callback = CheckpointCallback(
        save_freq=50_000 // NUM_ENVS,
        save_path=CHECKPOINT_DIR,
        name_prefix="ppo_finetune",
        save_vecnormalize=True
    )
    tb_callback = TensorboardCallback()
    eval_callback = CustomEvalCallback(
        eval_env=test_env,
        check_freq=20000,
        log_dir="./tensorboard_log/",
        best_model_save_path=CHECKPOINT_DIR,
        verbose=1
    )
    callback_list = CallbackList([checkpoint_callback, tb_callback, eval_callback])

    # 5. ì¬í•™ìŠµ ì‹œì‘
    print("ğŸš€ Starting Fine-tuning...")
    # reset_num_timesteps=Falseë¥¼ í•˜ë©´ í…ì„œë³´ë“œ ìŠ¤í…ì´ ì´ì–´ì„œ ì°í™ë‹ˆë‹¤. 
    # Trueë¡œ í•˜ë©´ 0ë¶€í„° ë‹¤ì‹œ ì‹œì‘í•©ë‹ˆë‹¤. (ìƒˆë¡œìš´ ë¡œê·¸ í´ë”ë¥¼ ì“°ë¯€ë¡œ True ì¶”ì²œ)
    model.learn(total_timesteps=TOTAL_TIMESTEPS, callback=callback_list, reset_num_timesteps=True)
    
    print("ğŸ Fine-tuning Finished.")
    model.save(os.path.join(CHECKPOINT_DIR, "final_finetuned_model"))
    train_env.save(os.path.join(CHECKPOINT_DIR, "final_vecnormalize.pkl"))

if __name__ == "__main__":
    main()