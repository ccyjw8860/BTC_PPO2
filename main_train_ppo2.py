import os
import sys
import numpy as np
import matplotlib.pyplot as plt
import gymnasium as gym

from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback, CallbackList
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor

import torch as th
import torch.nn as nn

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€ (í•„ìš” ì‹œ ìˆ˜ì •)
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# ì‚¬ìš©ì ì •ì˜ ëª¨ë“ˆ ì„í¬íŠ¸
# [ì£¼ì˜] trading_env3.py íŒŒì¼ëª…ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
from utils.rl_data.trading_env3 import TradingEnv 
from utils.rl_data.data_generator import RLDataGenerator

# 1. ì»¤ìŠ¤í…€ 1D CNN í´ë˜ìŠ¤ ì •ì˜
class Custom1DCNN(BaseFeaturesExtractor):
    """
    Time Seriesìš© 1D CNN Feature Extractor
    """
    def __init__(self, observation_space, features_dim=256):
        super().__init__(observation_space, features_dim)
        
        # ì…ë ¥ ì°¨ì›: (Batch, Seq_Len, N_Features) -> (Batch, N_Features, Seq_Len)ìœ¼ë¡œ ë³€í™˜í•´ ì²˜ë¦¬
        n_input_channels = observation_space.shape[1] # 59ê°œ Feature
        
        self.cnn = nn.Sequential(
            # Layer 1: ì„¸ë°€í•œ íŠ¹ì§• ì¶”ì¶œ
            nn.Conv1d(n_input_channels, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=2), # 100 -> 50
            
            # Layer 2: ì¡°ê¸ˆ ë” í° íŒ¨í„´ ì¶”ì¶œ
            nn.Conv1d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=2), # 50 -> 25
            
            # Layer 3: ì¶”ì„¸ ë“± ê±°ì‹œì  íŠ¹ì§• ì¶”ì¶œ
            nn.Conv1d(64, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=2), # 25 -> 12
            
            nn.Flatten(),
        )

        # CNN í†µê³¼ í›„ ì°¨ì› ê³„ì‚°
        with th.no_grad():
            # ë”ë¯¸ ë°ì´í„° ìƒì„± (Batch=1, Seq=100, Feat=59)
            sample = th.as_tensor(observation_space.sample()[None]).float()
            # PyTorch Conv1dëŠ” (Batch, Channel, Length) ìˆœì„œë¥¼ ì›í•¨ -> Permute
            sample = sample.permute(0, 2, 1)
            n_flatten = self.cnn(sample).shape[1]

        self.linear = nn.Sequential(
            nn.Linear(n_flatten, features_dim),
            nn.ReLU()
        )

    def forward(self, observations: th.Tensor) -> th.Tensor:
        # PPO ì…ë ¥ (Batch, Seq, Feat) -> (Batch, Feat, Seq)ë¡œ ìˆœì„œ ë³€ê²½
        x = observations.permute(0, 2, 1)
        x = self.cnn(x)
        return self.linear(x)


# -----------------------------------------------------------------------------
# 1. Custom Callback for TensorBoard (Train Log)
# -----------------------------------------------------------------------------
class TensorboardCallback(BaseCallback):
    """
    Custom callback for plotting additional values in tensorboard.
    í•™ìŠµ(Train) ì¤‘ì˜ Equity, Log Equity ë“±ì„ ê¸°ë¡í•©ë‹ˆë‹¤.
    """
    def __init__(self, verbose=0):
        super().__init__(verbose)

    def _on_step(self) -> bool:
        # PPOì˜ DummyVecEnvëŠ” infosë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
        # infos[0]ì— ì ‘ê·¼í•˜ì—¬ ì²« ë²ˆì§¸ í™˜ê²½ì˜ ê°’ì„ ê¸°ë¡í•©ë‹ˆë‹¤.
        infos = self.locals.get("infos", [{}])[0]
        
        if "equity_usd" in infos:
            self.logger.record("custom/equity_usd", infos["equity_usd"])
        if "log_equity" in infos:
            self.logger.record("custom/log_equity", infos["log_equity"])
            
        return True

# -----------------------------------------------------------------------------
# 2. Custom Callback for Evaluation (Eval Log) - ğŸŸ¢ [ì‹ ê·œ ì¶”ê°€]
# -----------------------------------------------------------------------------
class CustomEvalCallback(BaseCallback):
    """
    ì£¼ê¸°ì ìœ¼ë¡œ Test í™˜ê²½ì—ì„œ ëª¨ë¸ì„ í‰ê°€í•˜ê³ , Final Equityë¥¼ TensorBoardì— ê¸°ë¡í•˜ëŠ” ì½œë°±
    """
    def __init__(self, eval_env, eval_freq=10000, deterministic=True, verbose=1):
        super().__init__(verbose)
        self.eval_env = eval_env
        self.eval_freq = eval_freq
        self.deterministic = deterministic

    def _on_step(self) -> bool:
        # eval_freq ì£¼ê¸°ë§ˆë‹¤ í‰ê°€ ìˆ˜í–‰
        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:
            # evaluate_model í•¨ìˆ˜ë¥¼ ì¬ì‚¬ìš©í•˜ì—¬ í‰ê°€ ìˆ˜í–‰
            # (eval_envëŠ” VecNormalizeê°€ ì ìš©ëœ ìƒíƒœì—¬ì•¼ í•¨)
            equity_curve, final_equity = evaluate_model(self.model, self.eval_env, self.deterministic)
            
            # TensorBoard ê¸°ë¡ (eval íƒ­ì— í‘œì‹œë¨)
            self.logger.record("eval/final_equity_usd", final_equity)
            self.logger.record("eval/log_final_equity", np.log(max(final_equity, 1e-6)))
            
            # ì½˜ì†” ì¶œë ¥
            if self.verbose > 0:
                print(f"[CustomEval] Step {self.num_timesteps}: Final Equity = ${final_equity:,.2f}")
                
        return True

# -----------------------------------------------------------------------------
# 3. Environment Factory
# -----------------------------------------------------------------------------
def make_env(mode='train', sl_opts=None, tp_opts=None, window_size=100, max_episode_steps=2048):
    """
    í™˜ê²½ ìƒì„± íŒ©í† ë¦¬ í•¨ìˆ˜ (Binance ìˆ˜ìˆ˜ë£Œ ì ìš© ë²„ì „)
    """
    # 1. ë°ì´í„° ì œë„ˆë ˆì´í„° ìƒì„±
    # mode='train' -> train_x.npy ë¡œë“œ / mode='test' -> test_x.npy ë¡œë“œ
    data_gen = RLDataGenerator(mode=mode, seq_len=window_size)
    
    # 2. í™˜ê²½ ì´ˆê¸°í™”
    # fee_rate=0.0005 (0.05%) ì ìš©
    env = TradingEnv(
        data_generator=data_gen,
        sl_options=sl_opts,
        tp_options=tp_opts,
        window_size=window_size,
        pip_value=1.0,           # BTCUSDT 1 pip = 1 USDT
        lot_size=0.1,            # 1íšŒ ê±°ë˜ëŸ‰ 0.1 BTC
        spread_pips=10.0,        # ìŠ¤í”„ë ˆë“œ ë¹„ìš© ($10 ê°€ì •)
        fee_rate=0.0005,         # ê±°ë˜ ëŒ€ê¸ˆì˜ 0.05% (Binance Taker)
        max_slippage_pips=5.0,   # ìŠ¬ë¦¬í”¼ì§€ ìµœëŒ€ $5
        reward_scale=1.0,        # ë³´ìƒ ìŠ¤ì¼€ì¼
        initial_balance=10000.0, # ì´ˆê¸° ìë³¸ $10,000
        max_episode_steps=max_episode_steps, # ì—í”¼ì†Œë“œ ê¸¸ì´ ì œí•œ
        mode=mode
    )
    return env

# -----------------------------------------------------------------------------
# 4. Evaluation Function
# -----------------------------------------------------------------------------
def evaluate_model(model: PPO, eval_env: VecNormalize, deterministic: bool = True):
    """
    ëª¨ë¸ í‰ê°€ ë° Equity Curve ìƒì„±
    """
    # ì •ê·œí™” í†µê³„ ì—…ë°ì´íŠ¸ ì¤‘ì§€ (í‰ê°€ ëª¨ë“œ)
    # ë§¤ìš° ì¤‘ìš”: Test ì‹œì—ëŠ” í•™ìŠµ ë°ì´í„°ì˜ í†µê³„(mean, var)ë¥¼ ê³ ì •í•´ì„œ ì‚¬ìš©í•´ì•¼ í•¨
    eval_env.training = False
    eval_env.norm_reward = False
    
    obs = eval_env.reset()
    equity_curve = []
    
    # ì²« Equity ê¸°ë¡ (ì´ˆê¸° ìë³¸ê¸ˆ)
    current_equity = 10000.0 
    equity_curve.append(current_equity)

    done = False
    while not done:
        action, _ = model.predict(obs, deterministic=deterministic)
        obs, rewards, dones, infos = eval_env.step(action)
        
        # ì •ë³´ ì¶”ì¶œ
        info = infos[0]
        current_equity = info.get("equity_usd", current_equity)
        equity_curve.append(current_equity)
        
        done = dones[0]

    final_equity = float(equity_curve[-1])
    
    # í‰ê°€ê°€ ëë‚˜ë©´ ë‹¤ì‹œ Training ëª¨ë“œë¡œ ë³µêµ¬í•  í•„ìš”ëŠ” ì—†ìŒ (DummyVecEnvê°€ ë³„ë„ ê°ì²´ì´ë¯€ë¡œ)
    # í•˜ì§€ë§Œ ë§Œì•½ ë™ì¼ í™˜ê²½ì„ ì“´ë‹¤ë©´ ë³µêµ¬í•´ì•¼ í•¨. ì—¬ê¸°ì„  ë³„ë„ eval_envë¥¼ ì“°ë¯€ë¡œ ê´œì°®ìŒ.
    
    return equity_curve, final_equity

# -----------------------------------------------------------------------------
# 5. Main Training Loop
# -----------------------------------------------------------------------------
def main():
    # ---- A. í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ----
    # í¼ì„¼íŠ¸ ê¸°ë°˜ SL/TP ì˜µì…˜ (0.01 = 1%)
    SL_OPTS = [0.002, 0.005, 0.01, 0.02] 
    TP_OPTS = [0.005, 0.01, 0.02, 0.04]
    
    WINDOW_SIZE = 100          # ë°ì´í„° ì œë„ˆë ˆì´í„° seq_lenê³¼ ì¼ì¹˜í•´ì•¼ í•¨
    
    # [ì„¤ì •] ë©€í‹° í™˜ê²½ ë° ì—í”¼ì†Œë“œ ê¸¸ì´
    NUM_ENVS = 8               # ë³‘ë ¬ í™˜ê²½ ê°œìˆ˜ (CPU ì½”ì–´ ìˆ˜ì— ë§ì¶° 4~16 ê¶Œì¥)
    EPISODE_LENGTH = 2048      # ì—í”¼ì†Œë“œ ê¸¸ì´ (PPO n_stepsì™€ ì¼ì¹˜ ê¶Œì¥)
    
    TOTAL_TIMESTEPS = 50_000_000  # ì´ í•™ìŠµ ìŠ¤í… ìˆ˜
    ENT_COEF = 0.1
    GAMMA = 0.99
    GAE_LAMBDA = 0.9
    N_EPOCHS = 3
    CHECKPOINT_DIR = "./checkpoints_CNN"
    BATCH_SIZE = 2048

    print(f"Dataset Loading... Window Size: {WINDOW_SIZE}")
    print(f"Configuration: {NUM_ENVS} Envs, {EPISODE_LENGTH} Max Steps, Fee Rate: 0.05%")

    # ---- B. í™˜ê²½ ìƒì„± (Train / Test) ----
    # í›ˆë ¨ìš©: ì—¬ëŸ¬ ê°œì˜ í™˜ê²½ ìƒì„± (List Comprehension) -> DummyVecEnv
    # (ê° í™˜ê²½ì€ ë…ë¦½ì ì¸ ëœë¤ ì‹œì‘ì ì„ ê°€ì§)
    env_fns = [lambda: make_env('train', SL_OPTS, TP_OPTS, WINDOW_SIZE, EPISODE_LENGTH) for _ in range(NUM_ENVS)]
    train_vec_env = DummyVecEnv(env_fns)
    
    # [ì¤‘ìš”] VecNormalize: ì…ë ¥ ì •ê·œí™” + ë³´ìƒ ì •ê·œí™” (í•™ìŠµìš©)
    # Raw Dataê°€ ë“¤ì–´ì˜¤ë¯€ë¡œ clip_obsë¥¼ ë„‰ë„‰í•˜ê²Œ ì„¤ì •
    train_env = VecNormalize(train_vec_env, norm_obs=True, norm_reward=True, clip_obs=100.0)

    # í…ŒìŠ¤íŠ¸ìš©: ë‹¨ì¼ í™˜ê²½ (ê²€ì¦ìš©)
    # EvalCallbackì—ì„œ ì‚¬ìš©í•  í™˜ê²½
    test_vec_env = DummyVecEnv([
            lambda: make_env(
                'test', 
                SL_OPTS, 
                TP_OPTS, 
                WINDOW_SIZE, 
                max_episode_steps=0  # ğŸŸ¢ 0ìœ¼ë¡œ ì„¤ì •í•˜ë©´ ì¤‘ë‹¨ ì—†ì´ ëê¹Œì§€ ê°‘ë‹ˆë‹¤.
            )
        ])
    # í…ŒìŠ¤íŠ¸ í™˜ê²½ì€ í•™ìŠµ í™˜ê²½ì˜ í†µê³„(mean, var)ë¥¼ ê³µìœ ë°›ì§€ ì•Šê³  ì‹œì‘í•˜ë˜, 
    # ì‹¤ì œ í‰ê°€ ì‹œì—ëŠ” ë¡œë“œëœ í†µê³„ë¥¼ ë®ì–´ì”Œìš¸ ì˜ˆì •ì…ë‹ˆë‹¤.
    # ì—¬ê¸°ì„œëŠ” ì¼ë‹¨ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    test_env = VecNormalize(test_vec_env, norm_obs=True, norm_reward=False, clip_obs=100.0, training=False)

    print("Environment setup complete with VecNormalize.")

    # 2. ì •ì±… í‚¤ì›Œë“œ(policy_kwargs) ì„¤ì •
    # MLP ëŒ€ì‹  ìœ„ì—ì„œ ë§Œë“  Custom1DCNNì„ ì‚¬ìš©í•œë‹¤ê³  ëª…ì‹œ
    POLICY_KWARGS = dict(
        features_extractor_class=Custom1DCNN,
        features_extractor_kwargs=dict(features_dim=256),
        net_arch=[dict(pi=[64, 64], vf=[64, 64])] # CNN ë’¤ì— ë¶™ëŠ” íŒë‹¨ìš© MLP
    )

    # ---- C. ëª¨ë¸ ì •ì˜ (PPO) ----
    model = PPO(
        policy="MlpPolicy",
        policy_kwargs=POLICY_KWARGS,
        env=train_env,
        verbose=1,
        learning_rate=3e-4,
        n_steps=EPISODE_LENGTH,  # ì—í”¼ì†Œë“œ ê¸¸ì´ì™€ ë§ì¶¤ (ë²„í¼ ìµœì í™”)
        batch_size=BATCH_SIZE,         # ë°°ì¹˜ ì‚¬ì´ì¦ˆ (ë©”ëª¨ë¦¬ì— ë”°ë¼ 64~4096 ì¡°ì ˆ ê°€ëŠ¥)
        n_epochs=N_EPOCHS,
        gamma=GAMMA,
        gae_lambda=GAE_LAMBDA,
        clip_range=0.2,
        ent_coef=ENT_COEF,           # íƒìƒ‰ì„ ìœ„í•œ ì—”íŠ¸ë¡œí”¼ ê³„ìˆ˜
        tensorboard_log="./tensorboard_log/",
        device="cuda"            # GPU ì‚¬ìš© ëª…ì‹œ
    )

    # ---- D. ì½œë°± ì„¤ì • ----
    ckpt_dir = CHECKPOINT_DIR
    os.makedirs(ckpt_dir, exist_ok=True)

    # 1. CheckpointCallback: ëª¨ë¸ ì €ì¥
    # save_freqëŠ” ì „ì²´ ìŠ¤í… ê¸°ì¤€ì´ë¯€ë¡œ í™˜ê²½ ìˆ˜ë¡œ ë‚˜ëˆ„ì–´ ì¤ë‹ˆë‹¤.
    checkpoint_callback = CheckpointCallback(
        save_freq=50_000 // NUM_ENVS,
        save_path=ckpt_dir,
        name_prefix="btc_ppo",
        save_vecnormalize=True   # ì •ê·œí™” í†µê³„ ì €ì¥ í•„ìˆ˜!
    )
    
    # 2. TensorboardCallback: í•™ìŠµ ë¡œê·¸(Train Equity) ê¸°ë¡
    tb_callback = TensorboardCallback()

    # 3. ğŸŸ¢ [ì‹ ê·œ] CustomEvalCallback: í‰ê°€ ë¡œê·¸(Eval Equity) ê¸°ë¡
    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì£¼ê¸°ì™€ ë§ì¶°ì„œ 5ë§Œ ìŠ¤í…ë§ˆë‹¤ í‰ê°€ ìˆ˜í–‰
    eval_callback = CustomEvalCallback(
        eval_env=test_env,
        eval_freq=100_000 // NUM_ENVS, 
        deterministic=True,
        verbose=1
    )
    
    # ì½œë°± ë¦¬ìŠ¤íŠ¸ ë³‘í•©
    callback_list = CallbackList([checkpoint_callback, tb_callback, eval_callback])

    # ---- E. í•™ìŠµ ì‹œì‘ ----
    print(f"Start Training for {TOTAL_TIMESTEPS} timesteps...")
    model.learn(total_timesteps=TOTAL_TIMESTEPS, callback=callback_list)
    print("Training finished.")

    # ìµœì¢… ëª¨ë¸ ë° ì •ê·œí™” í†µê³„ ì €ì¥
    model.save("model_btc_final")
    train_env.save("vec_normalize_final.pkl")
    print("Final model saved.")

    # ---- F. OOS(Out-of-Sample) í‰ê°€ ë° Best Model ì„ ì • ----
    print("\nEvaluating Checkpoints on Test Data...")
    
    best_equity = -np.inf
    best_path = None
    
    # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²€ìƒ‰
    ckpts = sorted(
        [f for f in os.listdir(ckpt_dir) if f.endswith(".zip") and f.startswith("btc_ppo")],
        key=lambda x: os.path.getmtime(os.path.join(ckpt_dir, x))
    )

    for ck in ckpts:
        ck_path = os.path.join(ckpt_dir, ck)
        # VecNormalize í†µê³„ íŒŒì¼ ê²½ë¡œ ì¶”ë¡ 
        vec_path = ck_path.replace(".zip", "_vecnormalize.pkl")
        
        try:
            # 1. ëª¨ë¸ ë¡œë“œ
            loaded_model = PPO.load(ck_path)
            
            # 2. ì •ê·œí™” í†µê³„ ë¡œë“œ ë° í…ŒìŠ¤íŠ¸ í™˜ê²½ì— ì ìš©
            if os.path.exists(vec_path):
                # ì €ì¥ëœ í†µê³„ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ í™˜ê²½ ìƒì„±
                eval_env = VecNormalize.load(vec_path, test_vec_env)
                eval_env.training = False # ì—…ë°ì´íŠ¸ ë„ê¸°
                eval_env.norm_reward = False # ë³´ìƒ ì •ê·œí™” ë„ê¸°
            else:
                # í†µê³„ íŒŒì¼ì´ ì—†ìœ¼ë©´ ìµœì¢… í•™ìŠµ í†µê³„ ì‚¬ìš© (Fallback)
                print(f"[Warning] No VecNormalize stats found for {ck}. Using final training stats.")
                eval_env = test_env 
            
            # 3. í‰ê°€ ìˆ˜í–‰
            _, final_eq = evaluate_model(loaded_model, eval_env)
            
            print(f"[Eval] {ck} -> Final Equity: ${final_eq:,.2f}")
            
            if final_eq > best_equity:
                best_equity = final_eq
                best_path = ck_path
                
        except Exception as e:
            print(f"[Skip] Could not evaluate {ck}: {e}")

    # Best Model ê²°ì •
    print("-" * 50)
    if best_path:
        print(f"ğŸ† Best Model found: {best_path}")
        print(f"   Final Equity: ${best_equity:,.2f}")
        
        # Best Model ë° í†µê³„ ë¡œë“œ
        final_model = PPO.load(best_path)
        best_vec_path = best_path.replace(".zip", "_vecnormalize.pkl")
        if os.path.exists(best_vec_path):
            final_eval_env = VecNormalize.load(best_vec_path, test_vec_env)
        else:
            final_eval_env = test_env
    else:
        print("Using Final Model as Best.")
        final_model = model
        final_eval_env = test_env

    final_eval_env.training = False
    final_eval_env.norm_reward = False

    # ---- G. ìµœì¢… ê²°ê³¼ ì‹œê°í™” (Linear & Log Scale) ----
    print("Generating Equity Curve for Best Model...")
    equity_curve_test, _ = evaluate_model(final_model, final_eval_env)

    # Subplot ìƒì„±: ìœ„ìª½ì€ Linear, ì•„ë˜ìª½ì€ Log Scale
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)
    
    # 1. Linear Scale Plot
    ax1.plot(equity_curve_test, label="Test Equity (Linear)", color='orange')
    ax1.axhline(y=10000, color='r', linestyle='--', label="Initial Balance")
    ax1.set_title(f"Equity Curve: Best Model (Linear Scale) - {os.path.basename(best_path) if best_path else 'Final'}")
    ax1.set_ylabel("Equity (USDT)")
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # 2. Log Scale Plot
    ax2.plot(equity_curve_test, label="Test Equity (Log)", color='green')
    ax2.axhline(y=10000, color='r', linestyle='--', label="Initial Balance")
    ax2.set_yscale('log')  # Yì¶• ë¡œê·¸ ìŠ¤ì¼€ì¼ ì„¤ì •
    ax2.set_title(f"Equity Curve: Best Model (Log Scale)")
    ax2.set_xlabel("Steps")
    ax2.set_ylabel("Equity (Log Scale)")
    ax2.legend()
    ax2.grid(True, alpha=0.3, which="both") # ì„¸ë¶€ ëˆˆê¸ˆ í‘œì‹œ

    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    main()