import os
import sys
import numpy as np
import matplotlib.pyplot as plt
import gymnasium as gym

from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€ (í•„ìš” ì‹œ ìˆ˜ì •)
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# ì‚¬ìš©ì ì •ì˜ ëª¨ë“ˆ ì„í¬íŠ¸
# (íŒŒì¼ëª…ì´ trading_env3.pyë¼ê³  ê°€ì •, ë³€ê²½ ì‹œ ìˆ˜ì • í•„ìš”)
from utils.rl_data.trading_env3 import TradingEnv 
from utils.rl_data.data_generator import RLDataGenerator

class TensorboardCallback(BaseCallback):
    """
    Custom callback for plotting additional values in tensorboard.
    """
    def __init__(self, verbose=0):
        super().__init__(verbose)

    def _on_step(self) -> bool:
        # PPOì˜ DummyVecEnvëŠ” infosë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
        # infos[0]ì— ì ‘ê·¼í•˜ì—¬ ê°’ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        infos = self.locals.get("infos", [{}])[0]
        
        if "equity_usd" in infos:
            self.logger.record("custom/equity_usd", infos["equity_usd"])
        if "log_equity" in infos:
            self.logger.record("custom/log_equity", infos["log_equity"])
            
        return True

def make_env(mode='train', sl_opts=None, tp_opts=None, window_size=100):
    """
    í™˜ê²½ ìƒì„± íŒ©í† ë¦¬ í•¨ìˆ˜
    """
    # 1. ë°ì´í„° ì œë„ˆë ˆì´í„° ìƒì„±
    # mode='train' -> train_x.npy ë¡œë“œ / mode='test' -> test_x.npy ë¡œë“œ
    data_gen = RLDataGenerator(mode=mode, seq_len=window_size)
    
    # 2. í™˜ê²½ ì´ˆê¸°í™”
    env = TradingEnv(
        data_generator=data_gen,
        sl_options=sl_opts,
        tp_options=tp_opts,
        window_size=window_size,
        pip_value=1.0,           # BTCUSDT 1 pip = 1 USDT
        lot_size=0.1,            # 1íšŒ ê±°ë˜ëŸ‰ 0.1 BTC
        spread_pips=10.0,        # ìŠ¤í”„ë ˆë“œ $10 ê°€ì •
        commission_pips=10.0,    # ìˆ˜ìˆ˜ë£Œ $10 ê°€ì •
        max_slippage_pips=5.0,   # ìŠ¬ë¦¬í”¼ì§€ ìµœëŒ€ $5
        reward_scale=1.0,        # ë³´ìƒ ìŠ¤ì¼€ì¼
        initial_balance=10000.0, # ì´ˆê¸° ìë³¸ $10,000
        mode=mode
    )
    return env

def evaluate_model(model: PPO, eval_env: VecNormalize, deterministic: bool = True):
    """
    ëª¨ë¸ í‰ê°€ ë° Equity Curve ìƒì„±
    """
    # ì •ê·œí™” í†µê³„ ì—…ë°ì´íŠ¸ ì¤‘ì§€ (í‰ê°€ ëª¨ë“œ)
    eval_env.training = False
    eval_env.norm_reward = False
    
    obs = eval_env.reset()
    equity_curve = []
    
    # ì²« Equity ê¸°ë¡
    # VecEnvëŠ” ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ infoë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ ì²« ë²ˆì§¸ í™˜ê²½ì˜ info ì‚¬ìš©
    # ì´ˆê¸°í™” ì§í›„ì—ëŠ” infoê°€ ì—†ìœ¼ë¯€ë¡œ ì´ˆê¸° ìë³¸ê¸ˆìœ¼ë¡œ ì‹œì‘
    current_equity = 10000.0 
    equity_curve.append(current_equity)

    done = False
    while not done:
        action, _ = model.predict(obs, deterministic=deterministic)
        obs, rewards, dones, infos = eval_env.step(action)
        
        # ì •ë³´ ì¶”ì¶œ
        info = infos[0]
        current_equity = info.get("equity_usd", current_equity)
        equity_curve.append(current_equity)
        
        done = dones[0]

    final_equity = float(equity_curve[-1])
    return equity_curve, final_equity

def main():
    # ---- 1. ì„¤ì • ë° íŒŒë¼ë¯¸í„° ----
    # í¼ì„¼íŠ¸ ê¸°ë°˜ SL/TP ì˜µì…˜ (0.01 = 1%)
    SL_OPTS = [0.002, 0.005, 0.01, 0.02, 0.05] 
    TP_OPTS = [0.005, 0.01, 0.02, 0.04, 0.08]
    WINDOW_SIZE = 100  # ë°ì´í„° ì œë„ˆë ˆì´í„° seq_lenê³¼ ì¼ì¹˜í•´ì•¼ í•¨
    TOTAL_TIMESTEPS = 50_000_000  # í•™ìŠµ ìŠ¤í… ìˆ˜

    print(f"Dataset Loading... Window Size: {WINDOW_SIZE}")

    # ---- 2. í™˜ê²½ ìƒì„± (Train / Test) ----
    # í›ˆë ¨ìš© í™˜ê²½ (VecNormalize ì ìš©)
    # PPOëŠ” ë³‘ë ¬ í™˜ê²½ì„ ì§€ì›í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” DummyVecEnv(ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤) ì‚¬ìš©
    train_vec_env = DummyVecEnv([lambda: make_env('train', SL_OPTS, TP_OPTS, WINDOW_SIZE)])
    
    # [ì¤‘ìš”] VecNormalize: ì…ë ¥(Obs) ì •ê·œí™” + ë³´ìƒ(Reward) ì •ê·œí™”
    # Raw Dataê°€ ë“¤ì–´ì˜¤ë¯€ë¡œ clip_obsë¥¼ ë„‰ë„‰í•˜ê²Œ(100.0) ì„¤ì •í•˜ê±°ë‚˜ ê¸°ë³¸ê°’(10.0) ì‚¬ìš©
    train_env = VecNormalize(train_vec_env, norm_obs=True, norm_reward=True, clip_obs=100.0)

    # í…ŒìŠ¤íŠ¸(ê²€ì¦)ìš© í™˜ê²½
    # ì£¼ì˜: í…ŒìŠ¤íŠ¸ í™˜ê²½ì€ Train í™˜ê²½ì˜ í†µê³„(mean, var)ë¥¼ ê³µìœ ë°›ì•„ì•¼ í•¨ (ë’¤ì—ì„œ ì²˜ë¦¬)
    test_vec_env = DummyVecEnv([lambda: make_env('test', SL_OPTS, TP_OPTS, WINDOW_SIZE)])
    test_env = VecNormalize(test_vec_env, norm_obs=True, norm_reward=False, clip_obs=100.0, training=False)

    print("Environment setup complete with VecNormalize.")

    # ---- 3. ëª¨ë¸ ì •ì˜ (PPO) ----
    model = PPO(
        policy="MlpPolicy",
        env=train_env,
        verbose=1,
        learning_rate=3e-4,
        n_steps=2048,
        batch_size=64,
        n_epochs=10,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        ent_coef=0.01,
        tensorboard_log="./tensorboard_log/",
        device="cuda"  # ì—¬ê¸°ì— ëª…ì‹œì ìœ¼ë¡œ cuda ì§€ì • (ì„ íƒì‚¬í•­)
    )

    # ---- 4. ì²´í¬í¬ì¸íŠ¸ ì½œë°± ì„¤ì • ----
    ckpt_dir = "./checkpoints"
    os.makedirs(ckpt_dir, exist_ok=True)

    # save_vecnormalize=True: ì²´í¬í¬ì¸íŠ¸ë§ˆë‹¤ ì •ê·œí™” í†µê³„ë„ ê°™ì´ ì €ì¥ (í•„ìˆ˜!)
    checkpoint_callback = CheckpointCallback(
        save_freq=50_000,
        save_path=ckpt_dir,
        name_prefix="btc_ppo",
        save_vecnormalize=True 
    )

    tb_callback = TensorboardCallback()

    # ---- 5. í•™ìŠµ ì‹œì‘ ----
    print(f"Start Training for {TOTAL_TIMESTEPS} timesteps...")
    model.learn(total_timesteps=TOTAL_TIMESTEPS, callback=[checkpoint_callback, tb_callback])
    print("Training finished.")

    # ìµœì¢… ëª¨ë¸ ë° ì •ê·œí™” í†µê³„ ì €ì¥
    model.save("model_btc_final")
    train_env.save("vec_normalize_final.pkl")
    print("Final model saved.")

    # ---- 6. OOS(Out-of-Sample) í‰ê°€ ë° Best Model ì„ ì • ----
    print("\nEvaluating Checkpoints on Test Data...")

    # í…ŒìŠ¤íŠ¸ í™˜ê²½ì— ìµœì¢… í•™ìŠµëœ ì •ê·œí™” í†µê³„ ì ìš© (ì¼ë‹¨ ê¸°ë³¸ê°’ìœ¼ë¡œ)
    # ì‹¤ì œë¡œëŠ” ê° ì²´í¬í¬ì¸íŠ¸ì— ë§ëŠ” statsë¥¼ ë¡œë“œí•´ì•¼ í•¨
    
    best_equity = -np.inf
    best_path = None
    
    # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²€ìƒ‰ (zip íŒŒì¼)
    ckpts = sorted(
        [f for f in os.listdir(ckpt_dir) if f.endswith(".zip") and f.startswith("btc_ppo")],
        key=lambda x: os.path.getmtime(os.path.join(ckpt_dir, x))
    )

    for ck in ckpts:
        ck_path = os.path.join(ckpt_dir, ck)
        # VecNormalize í†µê³„ íŒŒì¼ ê²½ë¡œ ì¶”ë¡  (btc_ppo_X_steps.zip -> btc_ppo_X_steps_vecnormalize.pkl)
        # SB3 CheckpointCallbackì˜ ëª…ëª… ê·œì¹™ ë”°ë¦„
        vec_path = ck_path.replace(".zip", "_vecnormalize.pkl")
        
        try:
            # 1. ëª¨ë¸ ë¡œë“œ
            loaded_model = PPO.load(ck_path)
            
            # 2. ì •ê·œí™” í†µê³„ ë¡œë“œ ë° í…ŒìŠ¤íŠ¸ í™˜ê²½ì— ì ìš©
            if os.path.exists(vec_path):
                # ì €ì¥ëœ í†µê³„ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ í™˜ê²½ ìƒì„±
                eval_env = VecNormalize.load(vec_path, test_vec_env)
                eval_env.training = False # ì—…ë°ì´íŠ¸ ë„ê¸°
                eval_env.norm_reward = False # ë³´ìƒ ì •ê·œí™” ë„ê¸° (í‰ê°€ ì§€í‘œëŠ” ì‹¤ì œ ê¸ˆì•¡ì´ì–´ì•¼ í•¨)
            else:
                print(f"[Warning] No VecNormalize stats found for {ck}. Using final training stats.")
                eval_env = test_env # Fallback
            
            # 3. í‰ê°€ ìˆ˜í–‰
            _, final_eq = evaluate_model(loaded_model, eval_env)
            
            print(f"[Eval] {ck} -> Final Equity: ${final_eq:,.2f}")
            
            if final_eq > best_equity:
                best_equity = final_eq
                best_path = ck_path
                
        except Exception as e:
            print(f"[Skip] Could not evaluate {ck}: {e}")

    # Best Model ê²°ì •
    print("-" * 50)
    if best_path:
        print(f"ğŸ† Best Model found: {best_path}")
        print(f"   Final Equity: ${best_equity:,.2f}")
        
        # Best Model ë° í†µê³„ ë¡œë“œ
        final_model = PPO.load(best_path)
        best_vec_path = best_path.replace(".zip", "_vecnormalize.pkl")
        if os.path.exists(best_vec_path):
            final_eval_env = VecNormalize.load(best_vec_path, test_vec_env)
        else:
            final_eval_env = test_env
    else:
        print("Using Final Model as Best.")
        final_model = model
        final_eval_env = test_env

    final_eval_env.training = False
    final_eval_env.norm_reward = False

    # ---- 7. ìµœì¢… ê²°ê³¼ ì‹œê°í™” ----
    # Train êµ¬ê°„ í‰ê°€ (í•™ìŠµ ë°ì´í„°ì— ëŒ€í•œ ì„±ëŠ¥)
    # ì£¼ì˜: Train envëŠ” random startì´ë¯€ë¡œ ì „ì²´ ì»¤ë¸Œë¥¼ ê·¸ë¦¬ë ¤ë©´ deterministic ëª¨ë“œë¡œ ì²˜ìŒë¶€í„° ëê¹Œì§€ ëŒë ¤ì•¼ í•¨
    # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ Test ì…‹ì— ëŒ€í•´ì„œë§Œ ê·¸ë¦½ë‹ˆë‹¤.
    
    print("Generating Equity Curve for Best Model...")
    equity_curve_test, _ = evaluate_model(final_model, final_eval_env)

    plt.figure(figsize=(12, 6))
    plt.plot(equity_curve_test, label="Test (OOS) Equity", color='orange')
    plt.axhline(y=10000, color='r', linestyle='--', label="Initial Balance")
    plt.title(f"Equity Curve: Best Model ({os.path.basename(best_path) if best_path else 'Final'})")
    plt.xlabel("Steps")
    plt.ylabel("Equity (USDT)")
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    main()