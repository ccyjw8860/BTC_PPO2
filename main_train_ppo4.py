import os
import sys
import numpy as np
import matplotlib.pyplot as plt
import gymnasium as gym

from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback, CallbackList
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor

from utils.models.ppo_cnn2 import Custom1DCNN

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€ (í•„ìš” ì‹œ ìˆ˜ì •)
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# ì‚¬ìš©ì ì •ì˜ ëª¨ë“ˆ ì„í¬íŠ¸
# [ì£¼ì˜] trading_env3.py íŒŒì¼ëª…ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
from utils.rl_data.trading_env3 import TradingEnv 
from utils.rl_data.data_generator import RLDataGenerator

from datetime import datetime

def get_current_datetime():
    # í˜„ì¬ ë‚ ì§œì™€ ì‹œê°„ ê°€ì ¸ì˜¤ê¸°
    now = datetime.now()
    # ì›í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…í•˜ì—¬ ë°˜í™˜
    return now.strftime("%Y%m%d_%H%M")

# -----------------------------------------------------------------------------
# 1. Custom Callback for TensorBoard (Train Log)
# -----------------------------------------------------------------------------
class TensorboardCallback(BaseCallback):
    """
    Custom callback for plotting additional values in tensorboard.
    í•™ìŠµ(Train) ì¤‘ì˜ Equity, Log Equity ë“±ì„ ê¸°ë¡í•©ë‹ˆë‹¤.
    """
    def __init__(self, verbose=0):
        super().__init__(verbose)

    def _on_step(self) -> bool:
        # PPOì˜ DummyVecEnvëŠ” infosë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
        # infos[0]ì— ì ‘ê·¼í•˜ì—¬ ì²« ë²ˆì§¸ í™˜ê²½ì˜ ê°’ì„ ê¸°ë¡í•©ë‹ˆë‹¤.
        infos = self.locals.get("infos", [{}])[0]
        
        if "equity_usd" in infos:
            self.logger.record("custom/equity_usd", infos["equity_usd"])
        if "log_equity" in infos:
            self.logger.record("custom/log_equity", infos["log_equity"])
        if "open_position_length" in infos:
            self.logger.record("custom/open_position_length", infos["open_position_length"])
            
        return True

# -----------------------------------------------------------------------------
# 2. Custom Callback for Evaluation (Eval Log) - ğŸŸ¢ [ì‹ ê·œ ì¶”ê°€]
# -----------------------------------------------------------------------------
class CustomEvalCallback(BaseCallback):
    """
    í‰ê°€ í™˜ê²½ì—ì„œ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•˜ê³ , Equity ë“± ì»¤ìŠ¤í…€ ì§€í‘œë¥¼ ê¸°ë¡í•˜ë©°
    [ì¶”ê°€ë¨] 'ìµœì¢… ìì‚°(Final Equity)'ì´ ê°€ì¥ ë†’ì„ ë•Œ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤.
    """
    def __init__(self, eval_env, check_freq: int, log_dir: str, 
                 n_eval_episodes: int = 5, 
                 best_model_save_path: str = None,  # ğŸŸ¢ [ì¶”ê°€] ì €ì¥ ê²½ë¡œ ì¸ì
                 verbose=1):
        super(CustomEvalCallback, self).__init__(verbose)
        self.eval_env = eval_env
        self.check_freq = check_freq
        self.log_dir = log_dir
        self.n_eval_episodes = n_eval_episodes
        
        # ğŸŸ¢ [ì¶”ê°€] Best Model ì €ì¥ì„ ìœ„í•œ ë³€ìˆ˜
        self.best_model_save_path = best_model_save_path
        self.best_mean_equity = -np.inf
        
        # í…ì„œë³´ë“œ ë¡œê±°ê°€ ì—†ì„ ë•Œë¥¼ ëŒ€ë¹„í•œ ë¦¬ìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
        self.evaluations_equity = []

    def _init_callback(self) -> None:
        if self.best_model_save_path is not None:
            os.makedirs(self.best_model_save_path, exist_ok=True)

    def _on_step(self) -> bool:
        if self.n_calls % self.check_freq == 0:
            total_equity = 0.0
            valid_episodes = 0
            
            # --- í‰ê°€ ë£¨í”„ ---
            for _ in range(self.n_eval_episodes):
                obs = self.eval_env.reset()
                done = False
                while not done:
                    action, _ = self.model.predict(obs, deterministic=True)
                    obs, reward, done, infos = self.eval_env.step(action)
                    
                    if done:
                        info = infos[0]
                        # ìì‚° ì •ë³´ ì¶”ì¶œ
                        current_equity = 0.0
                        if 'equity_usd' in info:
                            current_equity = info['equity_usd']
                        elif 'terminal_observation' in info and 'equity_usd' in info.get('terminal_info', {}):
                            current_equity = info['terminal_info']['equity_usd']
                        
                        total_equity += current_equity
                        valid_episodes += 1
            
            # --- ê²°ê³¼ ê¸°ë¡ ë° ì €ì¥ ---
            if valid_episodes > 0:
                mean_equity = total_equity / valid_episodes
                
                # Tensorboard ê¸°ë¡
                self.logger.record("eval/mean_equity_usd", mean_equity)
                
                if self.verbose > 0:
                    print(f"Eval at step {self.num_timesteps}: Mean Equity = ${mean_equity:,.2f}")

                # ğŸŸ¢ [í•µì‹¬] Best Equity ê°±ì‹  ì‹œ ëª¨ë¸ ì €ì¥
                if self.best_model_save_path is not None:
                    if mean_equity > self.best_mean_equity:
                        if self.verbose > 0:
                            print(f"New Best Model! (Equity: ${self.best_mean_equity:,.2f} -> ${mean_equity:,.2f})")
                        
                        self.best_mean_equity = mean_equity
                        
                        # íŒŒì¼ëª…: best_model_equity.zip
                        save_file_path = os.path.join(self.best_model_save_path, "best_model_equity")
                        self.model.save(save_file_path)
                        
                    # í˜„ì¬ Best ìŠ¤ì½”ì–´ë„ ê¸°ë¡í•´ë‘ë©´ ì¢‹ìŒ
                    self.logger.record("eval/best_equity_usd", self.best_mean_equity)

        return True



# -----------------------------------------------------------------------------
# Environment Factory
# -----------------------------------------------------------------------------
def make_env(mode='train', sl_opts=None, tp_opts=None, window_size=100, max_episode_steps=2048, fee_rate=0.0005, slippage_rate=0.0):
    """
    fee_rate, slippage_rateë¥¼ ì¸ìë¡œ ë°›ì•„ í™˜ê²½ ìƒì„±
    """
    data_gen = RLDataGenerator(mode=mode, seq_len=window_size)
    
    env = TradingEnv(
        data_generator=data_gen,
        sl_options=sl_opts,
        tp_options=tp_opts,
        window_size=window_size,
        pip_value=1.0,
        lot_size=0.1,
        spread_pips=10.0,
        fee_rate=fee_rate,          #
        slippage_rate=slippage_rate, # ğŸŸ¢ [ìˆ˜ì •] max_slippage_pips -> slippage_rate êµì²´
        reward_scale=1.0,
        initial_balance=10000.0,
        max_episode_steps=max_episode_steps,
        mode=mode
    )
    return env
# -----------------------------------------------------------------------------
# 4. Evaluation Function
# -----------------------------------------------------------------------------
def evaluate_model(model: PPO, eval_env: VecNormalize, deterministic: bool = True):
    """
    ëª¨ë¸ í‰ê°€ ë° Equity Curve ìƒì„±
    """
    # ì •ê·œí™” í†µê³„ ì—…ë°ì´íŠ¸ ì¤‘ì§€ (í‰ê°€ ëª¨ë“œ)
    # ë§¤ìš° ì¤‘ìš”: Test ì‹œì—ëŠ” í•™ìŠµ ë°ì´í„°ì˜ í†µê³„(mean, var)ë¥¼ ê³ ì •í•´ì„œ ì‚¬ìš©í•´ì•¼ í•¨
    eval_env.training = False
    eval_env.norm_reward = False
    
    obs = eval_env.reset()
    equity_curve = []
    
    # ì²« Equity ê¸°ë¡ (ì´ˆê¸° ìë³¸ê¸ˆ)
    current_equity = 10000.0 
    equity_curve.append(current_equity)

    done = False
    while not done:
        action, _ = model.predict(obs, deterministic=deterministic)
        obs, rewards, dones, infos = eval_env.step(action)
        
        # ì •ë³´ ì¶”ì¶œ
        info = infos[0]
        current_equity = info.get("equity_usd", current_equity)
        equity_curve.append(current_equity)
        
        done = dones[0]

    final_equity = float(equity_curve[-1])
    
    # í‰ê°€ê°€ ëë‚˜ë©´ ë‹¤ì‹œ Training ëª¨ë“œë¡œ ë³µêµ¬í•  í•„ìš”ëŠ” ì—†ìŒ (DummyVecEnvê°€ ë³„ë„ ê°ì²´ì´ë¯€ë¡œ)
    # í•˜ì§€ë§Œ ë§Œì•½ ë™ì¼ í™˜ê²½ì„ ì“´ë‹¤ë©´ ë³µêµ¬í•´ì•¼ í•¨. ì—¬ê¸°ì„  ë³„ë„ eval_envë¥¼ ì“°ë¯€ë¡œ ê´œì°®ìŒ.
    
    return equity_curve, final_equity

# -----------------------------------------------------------------------------
# 5. Main Training Loop
# -----------------------------------------------------------------------------
def main():
    # ---- A. í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ----
    # í¼ì„¼íŠ¸ ê¸°ë°˜ SL/TP ì˜µì…˜ (0.01 = 1%)
    SL_OPTS = [0.002, 0.005, 0.01, 0.02] 
    TP_OPTS = [0.005, 0.01, 0.02, 0.04]
    
    WINDOW_SIZE = 1000          # ë°ì´í„° ì œë„ˆë ˆì´í„° seq_lenê³¼ ì¼ì¹˜í•´ì•¼ í•¨
    
    # [ì„¤ì •] ë©€í‹° í™˜ê²½ ë° ì—í”¼ì†Œë“œ ê¸¸ì´
    NUM_ENVS = 8               # ë³‘ë ¬ í™˜ê²½ ê°œìˆ˜ (CPU ì½”ì–´ ìˆ˜ì— ë§ì¶° 4~16 ê¶Œì¥)
    EPISODE_LENGTH = 2048      # ì—í”¼ì†Œë“œ ê¸¸ì´ (PPO n_stepsì™€ ì¼ì¹˜ ê¶Œì¥)
    
    TOTAL_TIMESTEPS = 50_000_000  # ì´ í•™ìŠµ ìŠ¤í… ìˆ˜
    ENT_COEF = 0.01
    GAMMA = 0.99
    GAE_LAMBDA = 0.9
    N_EPOCHS = 3
    current_time = get_current_datetime()
    CHECKPOINT_DIR = f"./checkpoints/checkpoints_CNN_seq1000_{current_time}"
    BATCH_SIZE = 2048
    FEE_RATE = 0.0
    SLIPPAGE_RATE = 0.0

    print(f"Dataset Loading... Window Size: {WINDOW_SIZE}")
    print(f"Configuration: {NUM_ENVS} Envs, {EPISODE_LENGTH} Max Steps, Fee Rate: {FEE_RATE}%")

    # ---- B. í™˜ê²½ ìƒì„± (Train / Test) ----
    # í›ˆë ¨ìš©: ì—¬ëŸ¬ ê°œì˜ í™˜ê²½ ìƒì„± (List Comprehension) -> DummyVecEnv
    # (ê° í™˜ê²½ì€ ë…ë¦½ì ì¸ ëœë¤ ì‹œì‘ì ì„ ê°€ì§)
    env_fns = [lambda: make_env('train', SL_OPTS, TP_OPTS, WINDOW_SIZE, EPISODE_LENGTH, FEE_RATE, SLIPPAGE_RATE) for _ in range(NUM_ENVS)]
    train_vec_env = DummyVecEnv(env_fns)
    
    # [ì¤‘ìš”] VecNormalize: ì…ë ¥ ì •ê·œí™” + ë³´ìƒ ì •ê·œí™” (í•™ìŠµìš©)
    # Raw Dataê°€ ë“¤ì–´ì˜¤ë¯€ë¡œ clip_obsë¥¼ ë„‰ë„‰í•˜ê²Œ ì„¤ì •
    train_env = VecNormalize(train_vec_env, norm_obs=False, norm_reward=True, clip_obs=100.0)

    # í…ŒìŠ¤íŠ¸ìš©: ë‹¨ì¼ í™˜ê²½ (ê²€ì¦ìš©)
    # EvalCallbackì—ì„œ ì‚¬ìš©í•  í™˜ê²½
    test_vec_env = DummyVecEnv([
            lambda: make_env(
                'test', 
                SL_OPTS, 
                TP_OPTS, 
                WINDOW_SIZE, 
                max_episode_steps=0, 
                fee_rate=FEE_RATE,
                slippage_rate=SLIPPAGE_RATE
            )
        ])
    # í…ŒìŠ¤íŠ¸ í™˜ê²½ì€ í•™ìŠµ í™˜ê²½ì˜ í†µê³„(mean, var)ë¥¼ ê³µìœ ë°›ì§€ ì•Šê³  ì‹œì‘í•˜ë˜, 
    # ì‹¤ì œ í‰ê°€ ì‹œì—ëŠ” ë¡œë“œëœ í†µê³„ë¥¼ ë®ì–´ì”Œìš¸ ì˜ˆì •ì…ë‹ˆë‹¤.
    # ì—¬ê¸°ì„œëŠ” ì¼ë‹¨ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    test_env = VecNormalize(test_vec_env, norm_obs=False, norm_reward=False, clip_obs=100.0, training=False)

    print("Environment setup complete with VecNormalize.")

    # 2. ì •ì±… í‚¤ì›Œë“œ(policy_kwargs) ì„¤ì •
    # MLP ëŒ€ì‹  ìœ„ì—ì„œ ë§Œë“  Custom1DCNNì„ ì‚¬ìš©í•œë‹¤ê³  ëª…ì‹œ
    POLICY_KWARGS = dict(
        features_extractor_class=Custom1DCNN,
        features_extractor_kwargs=dict(features_dim=256),
        net_arch=[dict(pi=[64, 64], vf=[64, 64])] # CNN ë’¤ì— ë¶™ëŠ” íŒë‹¨ìš© MLP
    )

    # ---- C. ëª¨ë¸ ì •ì˜ (PPO) ----
    model = PPO(
        policy="MlpPolicy",
        policy_kwargs=POLICY_KWARGS,
        env=train_env,
        verbose=1,
        learning_rate=3e-4,
        n_steps=EPISODE_LENGTH,  # ì—í”¼ì†Œë“œ ê¸¸ì´ì™€ ë§ì¶¤ (ë²„í¼ ìµœì í™”)
        batch_size=BATCH_SIZE,         # ë°°ì¹˜ ì‚¬ì´ì¦ˆ (ë©”ëª¨ë¦¬ì— ë”°ë¼ 64~4096 ì¡°ì ˆ ê°€ëŠ¥)
        n_epochs=N_EPOCHS,
        gamma=GAMMA,
        gae_lambda=GAE_LAMBDA,
        clip_range=0.2,
        ent_coef=ENT_COEF,           # íƒìƒ‰ì„ ìœ„í•œ ì—”íŠ¸ë¡œí”¼ ê³„ìˆ˜
        tensorboard_log="./tensorboard_log/",
        device="cuda"            # GPU ì‚¬ìš© ëª…ì‹œ
    )

    # ---- D. ì½œë°± ì„¤ì • ----
    ckpt_dir = CHECKPOINT_DIR
    os.makedirs(ckpt_dir, exist_ok=True)

    # 1. CheckpointCallback: ëª¨ë¸ ì €ì¥
    # save_freqëŠ” ì „ì²´ ìŠ¤í… ê¸°ì¤€ì´ë¯€ë¡œ í™˜ê²½ ìˆ˜ë¡œ ë‚˜ëˆ„ì–´ ì¤ë‹ˆë‹¤.
    checkpoint_callback = CheckpointCallback(
        save_freq=50_000 // NUM_ENVS,
        save_path=ckpt_dir,
        name_prefix="btc_ppo",
        save_vecnormalize=True   # ì •ê·œí™” í†µê³„ ì €ì¥ í•„ìˆ˜!
    )

    
    # 2. TensorboardCallback: í•™ìŠµ ë¡œê·¸(Train Equity) ê¸°ë¡
    tb_callback = TensorboardCallback()

    # 3. ğŸŸ¢ [ì‹ ê·œ] CustomEvalCallback: í‰ê°€ ë¡œê·¸(Eval Equity) ê¸°ë¡
    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì£¼ê¸°ì™€ ë§ì¶°ì„œ 5ë§Œ ìŠ¤í…ë§ˆë‹¤ í‰ê°€ ìˆ˜í–‰
    eval_callback = CustomEvalCallback(
        eval_env=test_env,
        check_freq=20000,           # 20,000 ìŠ¤í…ë§ˆë‹¤ í‰ê°€
        log_dir="./tensorboard_log/",
        n_eval_episodes=5,          # 5íŒ í‰ê· 
        best_model_save_path=ckpt_dir, # ğŸŸ¢ ì—¬ê¸°ì— ê²½ë¡œë¥¼ ì£¼ë©´ ì•Œì•„ì„œ ì €ì¥í•¨
        verbose=1
    )
    
    # ì½œë°± ë¦¬ìŠ¤íŠ¸ ë³‘í•©
    callback_list = CallbackList([checkpoint_callback, tb_callback, eval_callback])

    # ---- E. í•™ìŠµ ì‹œì‘ ----
    print(f"Start Training for {TOTAL_TIMESTEPS} timesteps...")
    model.learn(total_timesteps=TOTAL_TIMESTEPS, callback=callback_list)
    print("Training finished.")

    # ìµœì¢… ëª¨ë¸ ë° ì •ê·œí™” í†µê³„ ì €ì¥
    model.save("model_btc_final")
    train_env.save("vec_normalize_final.pkl")
    print("Final model saved.")

    # ---- F. OOS(Out-of-Sample) í‰ê°€ ë° Best Model ì„ ì • ----
    print("\nEvaluating Checkpoints on Test Data...")
    
    best_equity = -np.inf
    best_path = None
    
    # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²€ìƒ‰
    ckpts = sorted(
        [f for f in os.listdir(ckpt_dir) if f.endswith(".zip") and f.startswith("btc_ppo")],
        key=lambda x: os.path.getmtime(os.path.join(ckpt_dir, x))
    )

    for ck in ckpts:
        ck_path = os.path.join(ckpt_dir, ck)
        # VecNormalize í†µê³„ íŒŒì¼ ê²½ë¡œ ì¶”ë¡ 
        vec_path = ck_path.replace(".zip", "_vecnormalize.pkl")
        
        try:
            # 1. ëª¨ë¸ ë¡œë“œ
            loaded_model = PPO.load(ck_path)
            
            # 2. ì •ê·œí™” í†µê³„ ë¡œë“œ ë° í…ŒìŠ¤íŠ¸ í™˜ê²½ì— ì ìš©
            if os.path.exists(vec_path):
                # ì €ì¥ëœ í†µê³„ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ í™˜ê²½ ìƒì„±
                eval_env = VecNormalize.load(vec_path, test_vec_env)
                eval_env.training = False # ì—…ë°ì´íŠ¸ ë„ê¸°
                eval_env.norm_reward = False # ë³´ìƒ ì •ê·œí™” ë„ê¸°
            else:
                # í†µê³„ íŒŒì¼ì´ ì—†ìœ¼ë©´ ìµœì¢… í•™ìŠµ í†µê³„ ì‚¬ìš© (Fallback)
                print(f"[Warning] No VecNormalize stats found for {ck}. Using final training stats.")
                eval_env = test_env 
            
            # 3. í‰ê°€ ìˆ˜í–‰
            _, final_eq = evaluate_model(loaded_model, eval_env)
            
            print(f"[Eval] {ck} -> Final Equity: ${final_eq:,.2f}")
            
            if final_eq > best_equity:
                best_equity = final_eq
                best_path = ck_path
                
        except Exception as e:
            print(f"[Skip] Could not evaluate {ck}: {e}")

    # Best Model ê²°ì •
    print("-" * 50)
    if best_path:
        print(f"ğŸ† Best Model found: {best_path}")
        print(f"   Final Equity: ${best_equity:,.2f}")
        
        # Best Model ë° í†µê³„ ë¡œë“œ
        final_model = PPO.load(best_path)
        best_vec_path = best_path.replace(".zip", "_vecnormalize.pkl")
        if os.path.exists(best_vec_path):
            final_eval_env = VecNormalize.load(best_vec_path, test_vec_env)
        else:
            final_eval_env = test_env
    else:
        print("Using Final Model as Best.")
        final_model = model
        final_eval_env = test_env

    final_eval_env.training = False
    final_eval_env.norm_reward = False

    # ---- G. ìµœì¢… ê²°ê³¼ ì‹œê°í™” (Linear & Log Scale) ----
    print("Generating Equity Curve for Best Model...")
    equity_curve_test, _ = evaluate_model(final_model, final_eval_env)

    # Subplot ìƒì„±: ìœ„ìª½ì€ Linear, ì•„ë˜ìª½ì€ Log Scale
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)
    
    # 1. Linear Scale Plot
    ax1.plot(equity_curve_test, label="Test Equity (Linear)", color='orange')
    ax1.axhline(y=10000, color='r', linestyle='--', label="Initial Balance")
    ax1.set_title(f"Equity Curve: Best Model (Linear Scale) - {os.path.basename(best_path) if best_path else 'Final'}")
    ax1.set_ylabel("Equity (USDT)")
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # 2. Log Scale Plot
    ax2.plot(equity_curve_test, label="Test Equity (Log)", color='green')
    ax2.axhline(y=10000, color='r', linestyle='--', label="Initial Balance")
    ax2.set_yscale('log')  # Yì¶• ë¡œê·¸ ìŠ¤ì¼€ì¼ ì„¤ì •
    ax2.set_title(f"Equity Curve: Best Model (Log Scale)")
    ax2.set_xlabel("Steps")
    ax2.set_ylabel("Equity (Log Scale)")
    ax2.legend()
    ax2.grid(True, alpha=0.3, which="both") # ì„¸ë¶€ ëˆˆê¸ˆ í‘œì‹œ

    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    main()